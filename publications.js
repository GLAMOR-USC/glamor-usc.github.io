var papers = [
    {
        "paper_title": "THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation",
        "authors": "<b>Wilbert Pumacay</b>, <b>Ishika Singh</b>, Jiafei Duan, Ranjay Krishna, <b>Jesse Thomason</b>, Dieter Fox",
        "venue": "arXiV Pre-Print",
        "link": '<a href=\"https://arxiv.org/abs/2402.08191\">Paper</a> / <a href=\"https://github.com/robot-colosseum/robot-colosseum\">Code</a> / <a href=\"https://robot-colosseum.github.io/\">Website</a>', 
        "year": "2024",
    },
    {
        "paper_title": "Which One? Leveraging Context Between Objects and Multiple Views for Language Grounding",
        "authors": "Chancharik Mitra, <b>Abrar Anwar</b>, Rodolfo Corona, Dan Klein, Trevor Darrell, <b>Jesse Thomason</b>",
        "venue": "NAACL 2024",
        "link": '<a href=\"https://arxiv.org/abs/2311.06694\">Paper</a> / <a href=\"https://github.com/rcorona/magic_snare/\">Code</a>', 
        "year": "2024",
    },
    {
        "paper_title": "Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks",
        "authors": "<b>Ting-Yun Chang</b>, <b>Jesse Thomason</b>, Robin Jia",
        "venue": "NAACL 2024",
        "link": '<a href=\"https://arxiv.org/abs/2311.09060\">Paper</a> / <a href=\"https://github.com/terarachang/MemPi\">Code</a>', 
        "year": "2024",
    },
    {
        "paper_title": "TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models",
        "authors": "<b>Ishika Singh</b>, David Traum, <b>Jesse Thomason</b>",
        "venue": "arXiV Pre-Print",
        "link": '<a href=\"https://arxiv.org/abs/2403.17246\">Paper</a> / <a href=\"https://glamor-usc.github.io/twostep/\">Website</a>', 
        "year": "Pre-Prints",
    },
    {
        "paper_title": "Selective \"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning",
        "authors": "<b>Tejas Srinivasan</b>, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, <b>Jesse Thomason</b>, Khyathi Raghavi Chandu",
        "venue": "arXiV Pre-Print",
        "link": '<a href=\"https://arxiv.org/abs/2402.15610\">Paper</a>', 
        "year": "Pre-Prints",
    },
    {
        "paper_title": "Efficient End-to-End Visual Document Understanding with Rationale Distillation",
        "authors": "<b>Wang Zhu</b>, Alekh Agarwal, Mandar Joshi, Robin Jia, <b>Jesse Thomason</b>, Kristina Toutanova",
        "venue": "NAACL 2024",
        "link": '<a href=\"https://arxiv.org/abs/2311.09612\">Paper</a>', 
        "year": "2024",
    },
    {
        "paper_title": "Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?",
        "authors": "<b>Wang Zhu</b>, <b>Ishika Singh</b>, Yuan Huang, Robin Jia, <b>Jesse Thomason</b>",
        "venue": "Workshop on Open-Domain Reasoning Under Multi-Modal Settings (O-DRUM), CVPR 2023",
        "link": '<a href=\"https://arxiv.org/abs/2311.17280\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Exploring Strategies for Efficient Real-World VLN Evaluation",
        "authors": "<b>Abrar Anwar</b>, <b>Rohan Gupta</b>, <b>Elle Szabo</b>, <b>Jesse Thomason</b>",
        "venue": "LangRob Workshop, CoRL 2023",
        "link": '<a href=\"https://openreview.net/forum?id=uABEHp6tjy\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering",
        "authors": "<b>Wang Zhu</b>, <b>Jesse Thomason</b>, Robin Jia",
        "venue": "EMNLP 2023",
        "link": '<a href=\"https://arxiv.org/abs/2305.14901\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Exploring Strategies for Modeling Sign Language Phonology",
        "authors": "<b>Lee Kezar</b>, <b>Riley Carlin</b>, <b>Tejas Srinivasan</b>, Zed Sehyr, Naomi Caselli, <b>Jesse Thomason</b>",
        "venue": "ESANN 2023",
        "link": '<a href=\"https://www.esann.org/sites/default/files/proceedings/2023/ES2023-83.pdf\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes",
        "authors": "<b>Lee Kezar</b>, Elana Pontecorvo, Adele Daniels, Connor Baer, Ruth Ferster, Lauren Berger, <b>Jesse Thomason</b>, Zed Sehyr, and Naomi Caselli",
        "venue": "ASSETS 2023",
        "link": '<a href=\"https://dl.acm.org/doi/pdf/10.1145/3597638.3608408\">Paper</a>, <a href=\"https://github.com/terarachang/MemPi\">Code</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Self-Supervised 3D Representation Learning for Robotics",
        "authors": "<b>Ishika Singh</b>, <b>Anthony Liang</b>, Mohit Shridhar, <b>Jesse Thomason</b>",
        "venue": "Pretraining for Robotics Workshop, ICRA 2023",
        "link": '<a href=\"https://openreview.net/pdf?id=Hqb3t4Jqrk\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "ViSaRL: Visual Reinforcement Learning Guided by Human Saliency",
        "authors": "<b>Anthony Liang</b>, <b>Jesse Thomason</b>, Erdem Bıyık",
        "venue": "Pretraining for Robotics Workshop, ICRA 2023",
        "link": '<a href=\"https://openreview.net/pdf?id=1BrsVITE4PD\">Paper</a> / <a href=\"https://sites.google.com/view/visarl\">Website</a>', 
        "year": "2023",
    },
    {
        "paper_title": "I2I: Initializing Adapters with Improvised Knowledge",
        "authors": "<b>Tejas Srinivasan</b>, <b>Furong Jia</b>, Mohammad Rostami, <b>Jesse Thomason</b>",
        "venue": "CoLLAs 2023",
        "link": '<a href=\"https://arxiv.org/abs/2304.02168\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Multimodal Speech Recognition for Language-Guided Embodied Agents",
        "authors": "<b>Allen Chang</b>, Xiaoyuan Zhu, Aarav Monga, Seoho Ahn, <b>Tejas Srinivasan</b>, <b>Jesse Thomason</b>",
        "venue": "Interspeech 2023",
        "link": '<a href=\"https://arxiv.org/abs/2302.14030\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Curriculum Learning for Data-Efficient Vision-Language Alignment",
        "authors": "<b>Tejas Srinivasan</b>, Xiang Ren, <b>Jesse Thomason</b>",
        "venue": "Workshop on Open-Domain Reasoning Under Multi-Modal Settings (O-DRUM), CVPR 2023",
        "link": '<a href=\"https://arxiv.org/abs/2207.14525\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Iterative Vision-and-Language Navigation",
        "authors": "Jacob Krantz, Shurjo Banerjee, <b>Wang Zhu</b>, Jason Corso, Peter Anderson, Stefan Lee, <b>Jesse Thomason</b>",
        "venue": "CVPR 2023",
        "link": '<a href=\"https://arxiv.org/abs/2210.03087\">Paper</a> / <a href=\"https://jacobkrantz.github.io/ivln\">Website</a> / <a href=\"https://github.com/Bill1235813/IVLN\">Code</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Improving Sign Recognition with Phonology",
        "authors": "<b>Lee Kezar</b>, <b>Jesse Thomason</b>, Zed Sevcikova Sehyr",
        "venue": "EACL 2023",
        "link": '<a href=\"https://arxiv.org/abs/2302.05759\">Paper</a>', 
        "year": "2023",
    },
    {
        "paper_title": "Transformer Adapters for Robot Learning",
        "authors": "<b>Anthony Liang</b>, <b>Ishika Singh</b>, Karl Pertsch, <b>Jesse Thomason</b>",
        "venue": "Pretraining for Robot Learning Workshop, CoRL 2022",
        "link": '<a href=\"https://openreview.net/forum?id=H--wvRYBmF\">Paper</a>', 
        "year": "2022",
    },
    {
        "paper_title": "Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems",
        "authors": "<b>Wang Zhu</b>, <b>Jesse Thomason</b>, Robin Jia",
        "venue": "EMNLP 2022",
        "link": '<a href=\"https://arxiv.org/abs/2210.15037\">Paper</a>', 
        "year": "2022",
    },
    {
        "paper_title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
        "authors": "<b>Ishika Singh</b>, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, <b>Jesse Thomason</b>, Animesh Garg",
        "venue": "ICRA 2023",
        "link": '<a href=\"https://arxiv.org/abs/2209.11302\">Paper</a> / <a href=\"https://progprompt.github.io/\">Website</a> / <a href=\"https://www.youtube.com/watch?v=cawHI_rYQoM\">Video</a>', 
        "year": "2023",
    },
    {
        "paper_title": "CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks",
        "authors": "<b>Tejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva</b>, Georgios Chochlakis, Mohammad Rostami, <b>Jesse Thomason</b>",
        "venue": "NeurIPS 2022 Datasets and Benchmarks",
        "link": '<a href=\"https://arxiv.org/abs/2206.09059\">Paper</a> / <a href=\"https://github.com/GLAMOR-USC/CLiMB\">Code</a>', 
        "year": "2022",
    },

]


